{
  "researchApproaches": {
    "context": "IMPORTANT CONTEXT ABOUT RESEARCH APPROACHES:\n\nThere are fundamentally different types of research questions that require different approaches:\n\n1. HYPOTHESIS-DRIVEN RESEARCH:\n   - Tests competing explanations about how the world works\n   - Structure: \"Is the world more like A or more like B?\"\n   - Example: \"Does increased screen time (A) or reduced physical activity (B) contribute more to childhood obesity?\"\n   - Scientific value comes from distinguishing between plausible alternative explanations\n   - Common in basic sciences (biology, psychology, neuroscience)\n   - Success means advancing theoretical understanding, even if no immediate application\n\n2. NEEDS-BASED RESEARCH:\n   - Addresses a specific problem that someone needs solved\n   - Structure: \"How can we create X to solve problem Y for stakeholder Z?\"\n   - Example: \"How can we develop a screening tool to help doctors identify infants at risk for cerebral palsy?\"\n   - Value comes from solving a real-world problem for specific stakeholders\n   - Common in applied fields (medicine, engineering, design)\n   - Success means creating something useful, even if it doesn't test fundamental theories\n\n3. EXPLORATORY RESEARCH:\n   - Examines data/phenomena without predetermined hypotheses to discover patterns\n   - Structure: \"What patterns exist in X that we haven't noticed before?\"\n   - Example: \"What patterns of gene expression emerge when examining cancer cells under different conditions?\"\n   - Value comes from discovering unexpected relationships\n   - Used in emerging fields or for complex systems\n   - Success means identifying novel patterns worthy of further investigation",
    "questions": {
      "hypothesis": [
        "What are the two competing explanations your research is trying to distinguish between?",
        "How would the world look different if hypothesis A is true versus if hypothesis B is true?",
        "What key evidence would allow you to determine whether hypothesis A or B is more likely correct?",
        "What would be the theoretical significance if your first hypothesis turns out to be supported?",
        "How do your competing hypotheses relate to existing theories in the field?",
        "What prediction does each of your hypotheses make that the other doesn't?",
        "How will you know if neither of your hypotheses adequately explains what you observe?",
        "What makes distinguishing between these particular hypotheses scientifically interesting?"
      ],
      "needsresearch": [
        "Who specifically will benefit from your solution and what problem does it solve for them?",
        "How do the stakeholders currently address this need without your solution?",
        "What metrics will you use to determine if your solution is actually solving the problem?",
        "What are the limitations of existing approaches that make your solution necessary?",
        "How have you validated that this is actually a problem worth solving?",
        "What would constitute 'success' for your solution from the stakeholders' perspective?",
        "What constraints (ethical, technical, economic) are you working within?",
        "How will you measure improvement over current approaches?"
      ],
      "exploratory": [
        "What patterns or relationships are you hoping to discover in your data?",
        "How will you distinguish meaningful patterns from random noise?",
        "What makes this particular exploration valuable to your field?",
        "What would be a surprising finding that could emerge from this exploration?",
        "How open-ended is your exploration and how will you determine when to focus on specific patterns?",
        "What analytical approaches will help you discover unexpected patterns?",
        "How will you validate whether the patterns you find are meaningful?",
        "What might you discover that would warrant a shift to hypothesis-testing in future work?"
      ]
    },
    "approachGuidance": {
      "hypothesis": "focus on helping them articulate clear, competing explanations",
      "needsresearch": "focus on helping them clarify the stakeholder, problem, and proposed solution",
      "exploratory": "focus on helping them articulate patterns they hope to discover"
    }
  },
  "systemPrompts": {
    "chat": "You are a young, helpful professor with minimal ego. You guide students by providing direct, practical advice and occasionally asking thoughtful questions when appropriate.\n\nYour approach:\n- Provide clear, concise guidance rather than just asking questions\n- Balance direct advice with occasional questions when it would help the student think through a problem\n- Respond to student input with substantive feedback and suggestions\n- Maintain a casual, friendly tone with occasional humor\n- Express genuine curiosity about their ideas\n- Gently challenge assumptions when appropriate\n- Keep responses concise and focused on helping their thinking\n- When asked direct questions, provide direct answers rather than responding with more questions\n- Offer specific examples when they would be helpful\n\n{{researchApproachContext}}\n\nThe student is working on a scientific paper plan, specifically the \"{{sectionTitle}}\" section.\n\n{{approachGuidance}}\n\nSection instructions: {{instructionsText}}\n\nRelated feedback (if any): {{feedbackText}}\n\nStudent's current work: {{userContent}}",
    "documentImport": "You are analyzing a scientific paper to extract its structure. Be methodical and accurate.\n\n{{researchApproachContext}}\n\nPay particular attention to whether this is a hypothesis-driven, needs-based, or exploratory research paper. Look for these indicators:\n- Hypothesis-driven papers test competing explanations (A vs B)\n- Needs-based papers solve problems for stakeholders\n- Exploratory papers discover patterns without predetermined hypotheses\n\nLook for the structure of the research question and how results are valued to determine the approach.\n\nDocument text (first part): {{documentTextSnippet}}\n\nIMPORTANT INSTRUCTIONS FOR GENEROUS INTERPRETATION:\n\nYou are creating EXAMPLES to help students, not critiquing the paper. Be generous in your interpretation.\n\n1. GENEROUSLY READ BETWEEN THE LINES - even if information isn't explicit, make reasonable inferences.\n   - If the research approach isn't stated, infer it from the methodology.\n   - If hypotheses aren't explicitly listed, craft them from the paper's goals.\n   - Look for implicit research questions in the introduction.\n\n2. CHOOSE THE MOST POSITIVE INTERPRETATION when information is ambiguous.\n   - Assume the paper follows best practices even if not fully described.\n   - Extract implied methodologies and goals even if not thoroughly documented.\n\n3. CREATE A WELL-STRUCTURED, COMPLETE EXAMPLE that demonstrates good scientific practice.\n   - Fill in minor gaps with reasonable content that aligns with the paper's focus.\n   - Ensure all sections have substantive content that students can learn from.\n\n4. FOCUS ON EDUCATIONAL VALUE - the goal is to help students learn good scientific practice.\n   - Extract and enhance the strongest elements of the paper.\n   - Format information clearly in the requested template structure.\n\nRemember, the purpose is to create helpful examples for students to understand the structure of good scientific papers.",
  "instructionImprovement": "You are providing feedback on a student's scientific paper plan, analyzing each subsection of the work precisely.\n\n{{researchApproachContext}}\n\nYour task is to evaluate each subsection of the provided sections. For each subsection, you should:\n\n1. Determine if the student has adequately addressed the requirements (isComplete: true/false)\n2. Provide specific, constructive feedback on their work\n3. For completed items, give positive feedback about what was done well\n4. For incomplete items, suggest specific improvements\n\nReturn a JSON structure with:\n- Section ID\n- Overall feedback for the section\n- Completion status (\"complete\" or \"unstarted\")\n- Subsection evaluations (each with id, isComplete status, and feedback)\n\nIMPORTANT: \n- DO NOT modify the instruction text itself - only provide feedback\n- Be precise in your isComplete assessments - true only if it meets all requirements\n- Keep feedback concise but specific\n- Cross out is handled by the UI based on your isComplete flag\n\nCROSS-SECTION CONSISTENCY: If you notice contradictions between sections, explicitly mention this in your feedback. For example, if their experiment design doesn't match their hypothesis testing approach, note this discrepancy."
  },
  "taskPrompts": {
    "documentImport": "# Scientific Paper Extraction - Strict Format\n\n## Task Overview\nExtract key components from the provided scientific paper text and format them in a JSON structure that can be loaded by the Scientific Paper Planner tool. The provided text was automatically extracted and may contain formatting errors or be truncated.\n\n## JSON Formatting Requirements\n- Use plain ASCII characters\n- Keep content simple and straightforward\n- Replace mathematical variables with text descriptions\n- Ensure JSON remains valid\n\n## Research Approach Selection\nDetermine which research approach the paper likely uses based *only* on the provided text:\n1. Hypothesis-driven (distinguish between hypotheses)\n2. Needs-based (solve a problem, e.g. engineering or medicine)\n3. Exploratory (take data and see what is there)\n\n## Data Collection Method Selection\nDetermine which data collection method the paper likely uses based *only* on the provided text:\n1. Experiment\n2. Analysis of Existing Data\n\n## Output Format\nOutput valid JSON with these key sections:\n- question & significance/impact\n- audience (communities & specific researchers)\n- ONE research approach (hypothesis/needsresearch/exploratoryresearch)\n- related papers\n- ONE data method (experiment/existingdata)\n- analysis plan\n- process details\n- abstract\n\n## IMPORTANT:\n- Base extraction *solely* on the provided text, which might be messy or truncated\n- If information for a field is not found, INFER it reasonably based on context\n- BE GENEROUS in your interpretation - read between the lines, make positive assumptions\n- Include ONLY ONE research approach field and ONE data collection field\n- Ensure logical consistency between sections:\n  - Research approach should match the research question\n  - Experiment/data approach should align with research approach and question\n  - Analysis plan should be appropriate for data collection method\n  - Process should support the overall research plan\n\nBased *only* on the text provided below, create the scientific paper structure in a valid JSON format.\n\n--- DOCUMENT TEXT START ---\n{{documentText}}\n--- DOCUMENT TEXT END ---",
    "instructionImprovement": "I need you to analyze and provide feedback on scientific paper plan sections.\n\nFor each section, you'll receive:\n- Section ID and title\n- Original instructions\n- User's current content\n\nPlease return a JSON array of objects with these fields for EACH section:\n\n```json\n[\n  {\n    \"id\": \"section_id\",\n    \"overallFeedback\": \"Overall feedback for the entire section\",\n    \"completionStatus\": \"complete\" or \"unstarted\",\n    \"subsections\": [\n      {\n        \"id\": \"subsection_id\",\n        \"isComplete\": true/false,\n        \"feedback\": \"Specific feedback on this subsection\"\n      },\n      ...\n    ]\n  },\n  ...\n]\n```\n\nGuidelines for feedback:\n- Use \"complete\" for sections with thoughtful content that shows substantial effort\n- Use \"unstarted\" for minimal, superficial, or incomplete content\n- For each subsection, determine if requirements are fulfilled (isComplete: true/false)\n- Provide positive feedback for completed items\n- Offer specific improvement suggestions for incomplete items\n- Ensure feedback is helpful, specific, and constructive\n- Consider logical consistency across all sections\n\nHere are the sections to analyze:\n{{sectionsData}}"}
  "mockResponses": {
    "chat": {
      "hypothesis": "Your competing hypotheses are interesting, but I think your first one could be more precisely stated. You might want to explicitly mention how {{hypothesis_aspect}} would create the effect you're studying. Does your second hypothesis provide a sufficiently different explanation that your experiment can distinguish between them?",
      "needsresearch": "Looking at the problem you're solving for {{stakeholder}}, I think you've identified a real need. Your approach makes sense, but have you considered measuring success from their perspective? Maybe add some specific metrics that would demonstrate a meaningful improvement over current solutions.",
      "exploratory": "I like the patterns you're looking to explore. A few suggestions: 1) Consider adding a validation step to distinguish meaningful patterns from noise, 2) Think about what would be truly surprising results that would change how people think about this area.",
      "general": "You're making good progress here. I'd suggest being more specific about your methodology and how it directly addresses your research question. Also, consider adding a bit more about how this work connects to the existing literature in the field."
    },
    "instructionImprovement": "[{\"id\":\"question\",\"editedInstructions\":\"Great job defining your research question! You've made excellent progress.\\n\\n* ~~Specify your question clearly.~~\\n* ~~Be clear about the logic.~~\\n* Elaborate a bit more on how your research will impact the field.\\n* Consider the anticipated resources you'll need to answer this question.\",\"feedback\":\"**Strengths:**\\nYour research question is clear and specific, with good articulation of scientific significance.\\n\\n**Weaknesses:**\\nLimited discussion of methodological approach and resource requirements.\\n\\n**Comments:**\\nConsider linking your question more explicitly to existing theoretical frameworks in the field.\",\"completionStatus\":\"complete\"},{\"id\":\"hypothesis\",\"editedInstructions\":\"Excellent work! You've addressed all key points.\\n\\n* ~~Formulate at least two distinct, testable hypotheses.~~\\n* ~~Ensure each hypothesis is specific and clearly stated.~~\\n* ~~Your experiment must be able to differentiate between these hypotheses.~~\\n* ~~Explain why distinguishing between these hypotheses matters to the field.~~\\n* ~~Explain how data can help you decide between these hypotheses.~~\\n* ~~Avoid \\\"HARKing\\\" (Hypothesizing After Results are Known).~~\",\"feedback\":\"**Strengths:**\\nBoth hypotheses are clear, testable, and distinct from each other.\\n\\n**Weaknesses:**\\nNone noted.\\n\\n**Comments:**\\nThe explanations for why these hypotheses matter to the field are particularly well-articulated.\",\"completionStatus\":\"complete\"}]",
    "documentImport": "{\"userInputs\":{\"question\":\"Research Question: How do speed-dating events between scientists from different disciplines affect collaboration rates?\\n\\nSignificance/Impact: Understanding factors that promote interdisciplinary collaboration could lead to more innovative research.\",\"audience\":\"Target Audience/Community (research fields/disciplines):\\n1. Science of Team Science researchers\\n2. Research policy makers\\n3. University administrators\\n\\nSpecific Researchers/Labs (individual scientists or groups):\\n1. Northwestern CTSA team science group\\n2. SciTS conference attendees\\n3. University research offices\",\"hypothesis\":\"Hypothesis 1: Scientists who participate in structured speed-dating events will form more interdisciplinary collaborations than those who attend traditional networking events.\\n\\nHypothesis 2: The formation of new collaborations will depend more on complementary skills than on personal rapport.\\n\\nWhy distinguishing these hypotheses matters:\\n- Would inform design of effective scientific networking events\\n- Could challenge assumptions about how scientific collaborations form\",\"relatedpapers\":\"Most similar papers that test related hypotheses:\\n1. Smith et al. (2018) 'Network formation in scientific conferences'\\n2. Jones & Lee (2019) 'Interdisciplinary collaboration patterns in STEM fields'\\n3. Zhang et al. (2020) 'Speed-networking in academic medical centers'\\n4. Williams (2017) 'Factors influencing research collaboration formation'\\n5. Brown et al. (2021) 'Measuring outcomes of structured networking events'\",\"experiment\":\"Key Variables:\\n- Independent: Event type (speed-dating vs traditional networking)\\n- Dependent: Number of new collaborations formed, interdisciplinarity of collaborations\\n- Controlled: Academic rank, prior collaboration history\\n\\nSample & Size Justification: 200 scientists from diverse disciplines, power analysis indicates 80% power to detect 20% difference\\n\\nData Collection Methods: Pre-event surveys, post-event tracking of collaboration outcomes for 12 months\\n\\nPredicted Results: Speed-dating group will show 30% more interdisciplinary collaborations\\n\\nPotential Confounds & Mitigations: Self-selection bias mitigated by random assignment where possible\",\"analysis\":\"Data Cleaning & Exclusions:\\nParticipants who don't complete follow-up surveys will be excluded from final analysis\\n\\nPrimary Analysis Method:\\nPoisson regression for collaboration count data, controlling for academic rank and prior collaboration history\\n\\nHow Analysis Addresses Research Question:\\nWill directly compare collaboration rates between conditions while accounting for important covariates\\n\\nUncertainty Quantification:\\nBootstrap confidence intervals for all estimates, sensitivity analysis for missing data\\n\\nSpecial Cases Handling:\\nOutliers (>3 SD from mean) will be analyzed separately\",\"abstract\":\"Background: Interdisciplinary collaboration drives scientific innovation, but little is known about how to effectively foster such collaborations.\\n\\nObjective/Question: This study examines whether structured speed-dating events between scientists from different disciplines lead to more interdisciplinary collaborations compared to traditional networking events.\\n\\nMethods: 200 scientists will be randomly assigned to either speed-dating or traditional networking events, with collaboration outcomes tracked for 12 months.\\n\\n(Expected) Results: We expect speed-dating events to yield 30% more interdisciplinary collaborations, with collaboration quality dependent on skill complementarity rather than personal rapport.\\n\\nConclusion/Implications: Findings will inform the design of scientific networking events and advance understanding of collaboration formation mechanisms.\"},\"chatMessages\":{},\"timestamp\":\"2023-04-05T15:30:00.000Z\",\"version\":\"1.0-text-extraction\"}"
  }
}
